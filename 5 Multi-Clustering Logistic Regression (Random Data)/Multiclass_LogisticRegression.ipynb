{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463cf8ae-a287-4c62-a802-750bd284e09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Model ===\n",
      "Weights:\n",
      "[[ 0.34170275  2.14025     0.51804724]\n",
      " [ 2.00078956  1.68144826 -0.68223782]]\n",
      "Bias:\n",
      "[10.07882439 10.32929956  9.59187605]\n",
      "Final Cross-Entropy Loss: 0.5109\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_epochs=50):\n",
    "        self.lr = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.loss_history = []\n",
    "        self.frames = []  # Store plots as frames in memory\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability trick\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "    \n",
    "    def fit(self, X, y, save_path=\"Multiclass_LogisticRegression.gif\"):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))  # Number of classes\n",
    "        \n",
    "        # One-hot encoding the labels\n",
    "        y_one_hot = np.eye(n_classes)[y]\n",
    "        \n",
    "        self.weights = np.zeros((n_features, n_classes)) + 1\n",
    "        self.bias = np.zeros(n_classes) + 10\n",
    "    \n",
    "        for epoch in range(self.n_epochs):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.softmax(linear_model)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y_one_hot))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y_one_hot, axis=0)\n",
    "\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "\n",
    "            # Compute categorical cross-entropy loss\n",
    "            loss = -np.mean(np.sum(y_one_hot * np.log(y_pred + 1e-9), axis=1))\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "            # Save plot\n",
    "            self._save_plot(X, y, epoch, n_classes)\n",
    "        \n",
    "        # Save GIF after training\n",
    "        imageio.mimsave(save_path, self.frames, duration=0.1)\n",
    "    \n",
    "    def _save_plot(self, X, y, epoch, n_classes):\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Decision boundary\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                             np.linspace(y_min, y_max, 100))\n",
    "        \n",
    "        Z = np.dot(np.c_[xx.ravel(), yy.ravel()], self.weights) + self.bias\n",
    "        Z = np.argmax(self.softmax(Z), axis=1)\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        ax[0].contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "        scatter = ax[0].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k')\n",
    "        legend1 = ax[0].legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "        ax[0].add_artist(legend1)\n",
    "\n",
    "        ax[0].set_title(f'Decision Boundary - Epoch {epoch+1}')\n",
    "        ax[0].set_xlabel('Feature 1')\n",
    "        ax[0].set_ylabel('Feature 2')\n",
    "        ax[0].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "        # Loss plot\n",
    "        ax[1].plot(range(epoch+1), self.loss_history[:epoch+1], 'b-')\n",
    "        ax[1].set_title('Training Loss (Categorical Cross-Entropy)')\n",
    "        ax[1].set_xlabel('Epoch')\n",
    "        ax[1].set_ylabel('Loss')\n",
    "        ax[1].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        image = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "        self.frames.append(image)\n",
    "        plt.close(fig)\n",
    "\n",
    "# Generate multiclass data\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_classes=3, \n",
    "                           n_clusters_per_class=1, n_redundant=0, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression(learning_rate=0.5, n_epochs=100)\n",
    "model.fit(X, y, save_path=\"Multiclass_LogisticRegression.gif\")\n",
    "\n",
    "print(\"\\n=== Final Model ===\")\n",
    "print(f\"Weights:\\n{model.weights}\")\n",
    "print(f\"Bias:\\n{model.bias}\")\n",
    "print(f\"Final Cross-Entropy Loss: {model.loss_history[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f27d3c-2fcc-43ba-9921-748329064f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
